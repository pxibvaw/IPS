{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1vALo6cbjK_oaW3EVbjiTydglzRo9KjHH",
      "authorship_tag": "ABX9TyPmhim6JLrqCa1pHyLD9wgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pxibvaw/IPS/blob/pxibvaw/Dogpoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfPPREg8UuQ4",
        "outputId": "d3f3c59b-ac0f-4ab1-f6c0-83e080b2b28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Distribution in train_ds:\n",
            "Class 0: 1264 samples\n",
            "Class 1: 1271 samples\n",
            "Class 2: 1270 samples\n",
            "Class 3: 1261 samples\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.2875 - loss: 1.4913\n",
            "Epoch 1: val_loss improved from inf to 1.44616, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 515ms/step - accuracy: 0.2877 - loss: 1.4909 - val_accuracy: 0.2458 - val_loss: 1.4462 - learning_rate: 1.0000e-05\n",
            "Epoch 2/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.4119 - loss: 1.2868\n",
            "Epoch 2: val_loss improved from 1.44616 to 1.41784, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 243ms/step - accuracy: 0.4120 - loss: 1.2866 - val_accuracy: 0.2917 - val_loss: 1.4178 - learning_rate: 1.0000e-05\n",
            "Epoch 3/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.4955 - loss: 1.1670\n",
            "Epoch 3: val_loss improved from 1.41784 to 1.37415, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.4957 - loss: 1.1668 - val_accuracy: 0.3250 - val_loss: 1.3741 - learning_rate: 1.0000e-05\n",
            "Epoch 4/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.5807 - loss: 1.0439\n",
            "Epoch 4: val_loss improved from 1.37415 to 1.02968, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 241ms/step - accuracy: 0.5808 - loss: 1.0437 - val_accuracy: 0.6167 - val_loss: 1.0297 - learning_rate: 1.0000e-05\n",
            "Epoch 5/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.6150 - loss: 0.9763\n",
            "Epoch 5: val_loss improved from 1.02968 to 0.89084, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 244ms/step - accuracy: 0.6151 - loss: 0.9761 - val_accuracy: 0.6667 - val_loss: 0.8908 - learning_rate: 1.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6655 - loss: 0.8693\n",
            "Epoch 6: val_loss improved from 0.89084 to 0.86939, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 239ms/step - accuracy: 0.6655 - loss: 0.8692 - val_accuracy: 0.6500 - val_loss: 0.8694 - learning_rate: 1.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.6778 - loss: 0.8329\n",
            "Epoch 7: val_loss did not improve from 0.86939\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 227ms/step - accuracy: 0.6779 - loss: 0.8328 - val_accuracy: 0.6500 - val_loss: 0.8804 - learning_rate: 1.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.7025 - loss: 0.7780\n",
            "Epoch 8: val_loss improved from 0.86939 to 0.79301, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 241ms/step - accuracy: 0.7026 - loss: 0.7779 - val_accuracy: 0.6875 - val_loss: 0.7930 - learning_rate: 1.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.7227 - loss: 0.7222\n",
            "Epoch 9: val_loss improved from 0.79301 to 0.77845, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 240ms/step - accuracy: 0.7227 - loss: 0.7222 - val_accuracy: 0.6958 - val_loss: 0.7785 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.7269 - loss: 0.7096\n",
            "Epoch 10: val_loss improved from 0.77845 to 0.76627, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 249ms/step - accuracy: 0.7269 - loss: 0.7095 - val_accuracy: 0.6792 - val_loss: 0.7663 - learning_rate: 1.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.7466 - loss: 0.6594\n",
            "Epoch 11: val_loss did not improve from 0.76627\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 242ms/step - accuracy: 0.7466 - loss: 0.6594 - val_accuracy: 0.6833 - val_loss: 0.7777 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.7645 - loss: 0.6122\n",
            "Epoch 12: val_loss improved from 0.76627 to 0.73472, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.7644 - loss: 0.6122 - val_accuracy: 0.6917 - val_loss: 0.7347 - learning_rate: 1.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.7644 - loss: 0.6075\n",
            "Epoch 13: val_loss improved from 0.73472 to 0.69541, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 242ms/step - accuracy: 0.7644 - loss: 0.6074 - val_accuracy: 0.7000 - val_loss: 0.6954 - learning_rate: 1.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.7861 - loss: 0.5786\n",
            "Epoch 14: val_loss improved from 0.69541 to 0.68599, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 244ms/step - accuracy: 0.7861 - loss: 0.5785 - val_accuracy: 0.6875 - val_loss: 0.6860 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.7807 - loss: 0.5675\n",
            "Epoch 15: val_loss did not improve from 0.68599\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 236ms/step - accuracy: 0.7807 - loss: 0.5674 - val_accuracy: 0.6917 - val_loss: 0.6989 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.7962 - loss: 0.5443\n",
            "Epoch 16: val_loss did not improve from 0.68599\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 241ms/step - accuracy: 0.7962 - loss: 0.5441 - val_accuracy: 0.6958 - val_loss: 0.6899 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8043 - loss: 0.5224\n",
            "Epoch 17: val_loss improved from 0.68599 to 0.66141, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 252ms/step - accuracy: 0.8043 - loss: 0.5223 - val_accuracy: 0.7083 - val_loss: 0.6614 - learning_rate: 1.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8155 - loss: 0.4941\n",
            "Epoch 18: val_loss improved from 0.66141 to 0.64819, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 251ms/step - accuracy: 0.8155 - loss: 0.4941 - val_accuracy: 0.7375 - val_loss: 0.6482 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.8159 - loss: 0.4834\n",
            "Epoch 19: val_loss did not improve from 0.64819\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 238ms/step - accuracy: 0.8159 - loss: 0.4833 - val_accuracy: 0.7333 - val_loss: 0.6521 - learning_rate: 1.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8279 - loss: 0.4444\n",
            "Epoch 20: val_loss did not improve from 0.64819\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 235ms/step - accuracy: 0.8279 - loss: 0.4444 - val_accuracy: 0.7292 - val_loss: 0.6848 - learning_rate: 1.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8403 - loss: 0.4275\n",
            "Epoch 21: val_loss did not improve from 0.64819\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 234ms/step - accuracy: 0.8403 - loss: 0.4275 - val_accuracy: 0.7250 - val_loss: 0.6590 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8493 - loss: 0.4136\n",
            "Epoch 22: val_loss improved from 0.64819 to 0.64528, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 240ms/step - accuracy: 0.8493 - loss: 0.4136 - val_accuracy: 0.7417 - val_loss: 0.6453 - learning_rate: 5.0000e-06\n",
            "Epoch 23/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8504 - loss: 0.3934\n",
            "Epoch 23: val_loss improved from 0.64528 to 0.62627, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 245ms/step - accuracy: 0.8505 - loss: 0.3934 - val_accuracy: 0.7458 - val_loss: 0.6263 - learning_rate: 5.0000e-06\n",
            "Epoch 24/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8622 - loss: 0.3878\n",
            "Epoch 24: val_loss did not improve from 0.62627\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 235ms/step - accuracy: 0.8622 - loss: 0.3878 - val_accuracy: 0.7542 - val_loss: 0.6503 - learning_rate: 5.0000e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8584 - loss: 0.3927\n",
            "Epoch 25: val_loss did not improve from 0.62627\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 234ms/step - accuracy: 0.8584 - loss: 0.3927 - val_accuracy: 0.7167 - val_loss: 0.6579 - learning_rate: 5.0000e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8572 - loss: 0.3764\n",
            "Epoch 26: val_loss did not improve from 0.62627\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 229ms/step - accuracy: 0.8572 - loss: 0.3763 - val_accuracy: 0.7333 - val_loss: 0.6615 - learning_rate: 5.0000e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.8568 - loss: 0.3825\n",
            "Epoch 27: val_loss did not improve from 0.62627\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 303ms/step - accuracy: 0.8568 - loss: 0.3825 - val_accuracy: 0.7542 - val_loss: 0.6337 - learning_rate: 2.5000e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8654 - loss: 0.3485\n",
            "Epoch 28: val_loss improved from 0.62627 to 0.62552, saving model to ./model/best_model.keras\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 242ms/step - accuracy: 0.8654 - loss: 0.3486 - val_accuracy: 0.7583 - val_loss: 0.6255 - learning_rate: 2.5000e-06\n",
            "Epoch 29/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8582 - loss: 0.3705\n",
            "Epoch 29: val_loss did not improve from 0.62552\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 237ms/step - accuracy: 0.8582 - loss: 0.3704 - val_accuracy: 0.7625 - val_loss: 0.6376 - learning_rate: 2.5000e-06\n",
            "Epoch 30/30\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8722 - loss: 0.3637\n",
            "Epoch 30: val_loss did not improve from 0.62552\n",
            "\u001b[1m159/159\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 239ms/step - accuracy: 0.8722 - loss: 0.3637 - val_accuracy: 0.7667 - val_loss: 0.6358 - learning_rate: 2.5000e-06\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.7777 - loss: 0.5698\n",
            "\n",
            "Test Accuracy: 0.77\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "[[27  0  0  3]\n",
            " [ 2 27  1  0]\n",
            " [ 3  5 20  2]\n",
            " [ 4  2  5 19]]\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Diarrhea       0.75      0.90      0.82        30\n",
            "Lack of Water       0.79      0.90      0.84        30\n",
            "       Normal       0.77      0.67      0.71        30\n",
            "    Soft Poop       0.79      0.63      0.70        30\n",
            "\n",
            "     accuracy                           0.78       120\n",
            "    macro avg       0.78      0.78      0.77       120\n",
            " weighted avg       0.78      0.78      0.77       120\n",
            "\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.7777 - loss: 0.5698\n",
            "\n",
            "Best Model Test Accuracy: 0.77\n",
            "\n",
            " [ëª¨ë¸ í™•ì¸ ì¤‘] ./model/best_model.keras\n",
            " íŒŒì¼ í¬ê¸° í™•ì¸ë¨: 50.75 MB\n",
            " ëª¨ë¸ ë¡œë”© ì„±ê³µ (ì‚¬ìš© ê°€ëŠ¥)\n",
            "\n",
            " [ëª¨ë¸ í™•ì¸ ì¤‘] ./model/final_model.keras\n",
            " íŒŒì¼ í¬ê¸° í™•ì¸ë¨: 50.74 MB\n",
            " ëª¨ë¸ ë¡œë”© ì„±ê³µ (ì‚¬ìš© ê°€ëŠ¥)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import collections\n",
        "\n",
        "# ì„¤ì •ê°’\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/train'\n",
        "valid_dir = '/content/drive/MyDrive/Colab Notebooks/valid'\n",
        "test_dir  = '/content/drive/MyDrive/Colab Notebooks/test'\n",
        "\n",
        "model_dir = './model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# ì ‘ë‘ì–´ â†’ í´ë˜ìŠ¤ ë§¤í•‘\n",
        "def get_label_from_filename(filename):\n",
        "    filename = tf.cast(filename, tf.string)\n",
        "    basename = tf.strings.split(filename, os.sep)[-1]\n",
        "    prefix = tf.strings.split(basename, '-')[0]\n",
        "\n",
        "    label = tf.case([\n",
        "        (tf.equal(prefix, 'D'), lambda: tf.constant(0)),\n",
        "        (tf.equal(prefix, 'LW'), lambda: tf.constant(1)),\n",
        "        (tf.equal(prefix, 'N'), lambda: tf.constant(2)),\n",
        "        (tf.equal(prefix, 'SP'), lambda: tf.constant(3)),\n",
        "    ], default=lambda: tf.constant(-1))\n",
        "\n",
        "    return label\n",
        "\n",
        "# ë°ì´í„° ì¦ê°• (ê°„ì†Œí™”)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.05),\n",
        "])\n",
        "\n",
        "# í•™ìŠµìš© ì „ì²˜ë¦¬\n",
        "def preprocess_image_train(filename):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.image.resize(image, img_size)\n",
        "    image = image / 255.0\n",
        "    image = data_augmentation(image)\n",
        "    label = get_label_from_filename(filename)\n",
        "    return image, label\n",
        "\n",
        "# í‰ê°€ìš© ì „ì²˜ë¦¬\n",
        "def preprocess_image_eval(filename):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.image.resize(image, img_size)\n",
        "    image = image / 255.0\n",
        "    label = get_label_from_filename(filename)\n",
        "    return image, label\n",
        "\n",
        "# ë°ì´í„°ì…‹ ìƒì„±\n",
        "def create_dataset_from_folder(folder_path, is_train=False):\n",
        "    file_types = ['*.jpg', '*.jpeg', '*.png']\n",
        "    filenames = []\n",
        "    for ext in file_types:\n",
        "        filenames += tf.io.gfile.glob(os.path.join(folder_path, ext))\n",
        "\n",
        "    valid_filenames = []\n",
        "    for f in filenames:\n",
        "        try:\n",
        "            img = tf.io.read_file(f)\n",
        "            tf.image.decode_image(img, channels=3)\n",
        "            valid_filenames.append(f)\n",
        "        except:\n",
        "            print(f\" Invalid image skipped: {f}\")\n",
        "\n",
        "    valid_filenames = np.array(valid_filenames, dtype=str)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(valid_filenames)\n",
        "    preprocess_fn = preprocess_image_train if is_train else preprocess_image_eval\n",
        "    dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ë°ì´í„° ë¡œë”©\n",
        "train_ds = create_dataset_from_folder(train_dir, is_train=True)\n",
        "valid_ds = create_dataset_from_folder(valid_dir, is_train=False)\n",
        "test_ds = create_dataset_from_folder(test_dir, is_train=False)\n",
        "\n",
        "# ë ˆì´ë¸” ë¶„í¬ ì¶œë ¥\n",
        "label_counts = collections.Counter()\n",
        "for _, label in train_ds.unbatch():\n",
        "    label_counts[int(label.numpy())] += 1\n",
        "print(\"Label Distribution in train_ds:\")\n",
        "for k, v in sorted(label_counts.items()):\n",
        "    print(f\"Class {k}: {v} samples\")\n",
        "\n",
        "# EfficientNetB0 ê¸°ë°˜ ëª¨ë¸\n",
        "def build_efficientnet_b0(input_shape=(128,128,3), num_classes=4, dropout_rate=0.5):\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    base_model.trainable = True  # ì „ì²´ í•™ìŠµ\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = base_model(inputs, training=True)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model, base_model\n",
        "\n",
        "# ì½œë°±\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"best_model.keras\"),\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode='min',\n",
        "        verbose=2\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    ),\n",
        "]\n",
        "\n",
        "# ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ\n",
        "model, base_model = build_efficientnet_b0(input_shape=img_size+(3,), num_classes=num_classes)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # ë‚®ì€ í•™ìŠµë¥ \n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.fit(train_ds, validation_data=valid_ds, epochs=30, callbacks=callbacks)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# ì •ë°€ ì„±ëŠ¥ ë¶„ì„\n",
        "y_true, y_pred = [], []\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=['Diarrhea','Lack of Water','Normal','Soft Poop']))\n",
        "\n",
        "\n",
        "# í•™ìŠµ ì™„ë£Œ í›„ best_model ë¡œë”©\n",
        "best_model = tf.keras.models.load_model(os.path.join(model_dir, \"best_model.keras\"))\n",
        "\n",
        "# í‰ê°€\n",
        "test_loss, test_acc = best_model.evaluate(test_ds)\n",
        "print(f\"\\nBest Model Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# ì €ì¥\n",
        "best_model.save(os.path.join(model_dir, \"final_model.keras\"))\n",
        "\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def verify_model_saved_and_loadable(model_path, min_size_kb=100):\n",
        "    print(f\"\\n [ëª¨ë¸ í™•ì¸ ì¤‘] {model_path}\")\n",
        "\n",
        "    # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\" ì €ì¥ ì‹¤íŒ¨: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return False\n",
        "\n",
        "    # 2. ìš©ëŸ‰ í™•ì¸\n",
        "    file_size = os.path.getsize(model_path)\n",
        "    file_size_mb = round(file_size / (1024 * 1024), 2)\n",
        "    if file_size < min_size_kb * 1024:\n",
        "        print(f\" íŒŒì¼ ì¡´ì¬í•˜ì§€ë§Œ ë„ˆë¬´ ì‘ìŒ: {file_size_mb} MB (ì˜ì‹¬ë¨)\")\n",
        "    else:\n",
        "        print(f\" íŒŒì¼ í¬ê¸° í™•ì¸ë¨: {file_size_mb} MB\")\n",
        "\n",
        "    # 3. ë¡œë”© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "    try:\n",
        "        _ = tf.keras.models.load_model(model_path)\n",
        "        print(\" ëª¨ë¸ ë¡œë”© ì„±ê³µ (ì‚¬ìš© ê°€ëŠ¥)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\" ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "verify_model_saved_and_loadable(os.path.join(model_dir, \"best_model.keras\"))\n",
        "verify_model_saved_and_loadable(os.path.join(model_dir, \"final_model.keras\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "\n",
        "# ì„¤ì •ê°’\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/train'\n",
        "valid_dir = '/content/drive/MyDrive/Colab Notebooks/valid'\n",
        "test_dir  = '/content/drive/MyDrive/Colab Notebooks/test'\n",
        "\n",
        "# ì ‘ë‘ì–´ â†’ í´ë˜ìŠ¤ ë§¤í•‘\n",
        "def get_label_from_filename(filename):\n",
        "    filename = tf.cast(filename, tf.string)\n",
        "    basename = tf.strings.split(filename, os.sep)[-1]\n",
        "    prefix = tf.strings.split(basename, '-')[0]\n",
        "\n",
        "    label = tf.case([\n",
        "        (tf.equal(prefix, 'D'), lambda: tf.constant(0)),\n",
        "        (tf.equal(prefix, 'LW'), lambda: tf.constant(1)),\n",
        "        (tf.equal(prefix, 'N'), lambda: tf.constant(2)),\n",
        "        (tf.equal(prefix, 'SP'), lambda: tf.constant(3)),\n",
        "    ], default=lambda: tf.constant(-1))\n",
        "\n",
        "    return label\n",
        "\n",
        "# ë°ì´í„° ì¦ê°• (ê°„ì†Œí™”)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.05),\n",
        "])\n",
        "\n",
        "# í•™ìŠµìš© ì „ì²˜ë¦¬\n",
        "def preprocess_image_train(filename):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.image.resize(image, img_size)\n",
        "    image = image / 255.0\n",
        "    image = data_augmentation(image)\n",
        "    label = get_label_from_filename(filename)\n",
        "    return image, label\n",
        "\n",
        "# í‰ê°€ìš© ì „ì²˜ë¦¬\n",
        "def preprocess_image_eval(filename):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.image.resize(image, img_size)\n",
        "    image = image / 255.0\n",
        "    label = get_label_from_filename(filename)\n",
        "    return image, label\n",
        "\n",
        "# ë°ì´í„°ì…‹ ìƒì„±\n",
        "def create_dataset_from_folder(folder_path, is_train=False):\n",
        "    file_types = ['*.jpg', '*.jpeg', '*.png']\n",
        "    filenames = []\n",
        "    for ext in file_types:\n",
        "        filenames += tf.io.gfile.glob(os.path.join(folder_path, ext))\n",
        "\n",
        "    valid_filenames = []\n",
        "    for f in filenames:\n",
        "        try:\n",
        "            img = tf.io.read_file(f)\n",
        "            tf.image.decode_image(img, channels=3)\n",
        "            valid_filenames.append(f)\n",
        "        except:\n",
        "            print(f\" Invalid image skipped: {f}\")\n",
        "\n",
        "    valid_filenames = np.array(valid_filenames, dtype=str)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(valid_filenames)\n",
        "    preprocess_fn = preprocess_image_train if is_train else preprocess_image_eval\n",
        "    dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ë°ì´í„° ë¡œë”©\n",
        "\n",
        "test_ds = create_dataset_from_folder(test_dir, is_train=False)\n",
        "\n",
        "\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model = tf.keras.models.load_model(\"model/best_model.keras\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',  # í‰ê°€\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ì˜ˆì¸¡\n",
        "y_true, y_pred = [], []\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "\n",
        "# í‰ê°€\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(\"\\nğŸ“Š Classification Report:\")\n",
        "class_names = ['Diarrhea','Lack of Water','Normal','Soft Poop']\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "\n",
        "# debug ì½”ë“œë¡œ í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¹„ìœ¨ì„ ì¶œë ¥í•´ë³´ì\n",
        "import collections\n",
        "print(collections.Counter(y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDfgQdJ2cPMr",
        "outputId": "e50e1e3f-9600-4b75-f283-5470128576fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.7777 - loss: 0.5698\n",
            "\n",
            "Test Accuracy: 0.77\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "[[27  0  0  3]\n",
            " [ 2 27  1  0]\n",
            " [ 3  5 20  2]\n",
            " [ 4  2  5 19]]\n",
            "\n",
            "ğŸ“Š Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Diarrhea       0.75      0.90      0.82        30\n",
            "Lack of Water       0.79      0.90      0.84        30\n",
            "       Normal       0.77      0.67      0.71        30\n",
            "    Soft Poop       0.79      0.63      0.70        30\n",
            "\n",
            "     accuracy                           0.78       120\n",
            "    macro avg       0.78      0.78      0.77       120\n",
            " weighted avg       0.78      0.78      0.77       120\n",
            "\n",
            "Counter({np.int64(0): 36, np.int64(1): 34, np.int64(2): 26, np.int64(3): 24})\n"
          ]
        }
      ]
    }
  ]
}